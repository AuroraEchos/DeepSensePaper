#### 主要贡献：

1. 提出一个基于深度学习的多模态感知波束预测方法，面向 5G-NR 标准，能够预测一组最优子集波束，而不是通过标准的穷举波束扫描。这种方式显著减少了波束搜索空间与扫描延迟，在 mmWave 高动态场景下尤其有用。
2. 综合利用多种感知信息（如车辆位置数据、不同传感器模态），以弥补单一模态在误差与信息缺失方面的不足。特别是使用位置数据来帮助识别城市场景中的目标车辆。
3. 方案依赖于普遍可用的车载和路侧传感器（非专用昂贵硬件），降低了部署复杂度与成本。
4. 实验数据不是仿真（如射线追踪），而是来自真实环境的 V2I 与 V2V 场景，考虑了车辆的移动性。通过 **预测精度（Accuracy）** 和 **接收功率比（Received Power Ratio）** 两个指标验证了方法有效性。
5. 与 5G-NR 标准兼容，并展示了集成后的好处：波束搜索空间减少 **79.67%**，搜索时间减少 **91.89%**（Top-13 波束情况下），在 Top-13 波束预测中准确率可达 **98.19%**，显著优于传统穷举扫描。

#### 采用方法：

作者的方法核心是基于多模态感知数据的深度学习波束预测框架，包含数据预处理、多模态特征提取和Top-M波束选择三个主要步骤，模型结构图如下：

<img src="./img/12_1.png" style="zoom: 50%;" />

**数据预处理：**

为了让不同模态的数据适配统一的神经网络输入，并降低计算复杂度，作者对三类感知数据进行了归一化、尺寸固定化等处理：

1. 针对位置数据，将经纬度从原来的十进制度数映射到$[-1, 1]$返回进行归一化，结果是一个在笛卡尔坐标系下的标准化矩阵。
2. 针对视觉数据，将图像尺寸统一为$224\times224\times3$，对RGB三个通道分别按照ImageNet标准均值和方差进行归一化（零均值、单位方差）。
3. 针对点云数据，将每帧点云统一为15000个点，不足则零填充，过多则随即下采样，保留三维坐标信息。

**多模态特征提取：**

每种模态独立经过一个深度学习子网络提取特征：

1. 位置信息特征提取使用一个一维神经网络结构，三个连续的 Conv1D + MaxPooling 块，用于逐步提取局部时序/空间特征，并降维防止过拟合，最后通过 Flatten 层将特征展平成一维向量。
2. 视觉特征的提取采用Fine-tuned EfficientNet-B0。
3. 点云特征的提取采用Fine-tuned PointNet。

**Top-M波束选择：**

将三种模态特征向量沿特征维拼接（Concatenate）成一个综合特征向量。经过两个全连接层（FCN1, FCN2）进行特征融合和非线性映射。通过**Softmax**得到 64 个候选波束的概率分布。选取概率最高的 **Top-M 波束** 作为预测结果，实现显著减少波束搜索空间和延迟。

#### 实验结果：

实验数据来源于 DeepSense 6G dataset。使用场景包括：

- **V2I**（车到基站）：Scenario 31, 32, 33, 34
- **V2V**（车到车）：Scenario 36, 37, 38, 39

样本数量：

- 31：7,012
- 32：3,235
- 33：3,981
- 34：4,439
- 36：24,800
- 37：31,000
- 38：36,000
- 39：20,400

数据划分：每个场景 60% 训练 / 20% 验证 / 20% 测试。

<img src="./img/12_2.png" style="zoom:67%;" />

由上述结果可以看出随 Epoch 增加逐渐下降，表示预测与真实波束差异减小，准确了曲线逐渐上升，说明模型学习到有效特征。

<img src="./img/12_3.png" style="zoom: 80%;" />

上述为不同场景下的 Top-K 准确率。

#### 局限性：

该方法虽然在 LoS 场景中效果显著，但在跨场景泛化、NLoS 适应、感知数据鲁棒性、端到端性能验证以及标准化集成等方面仍存在明显不足。

#### 代码复现记录报告

作者已经开源[代码](https://github.com/mbaqer/V2X-mmWave-Beamforming)。第一步将代码克隆到本地：

`git clone git@github.com:mbaqer/V2X-mmWave-Beamforming.git`

**该开源文件目录结构如下：**

V2X-mmWave-Beamforming

- Codes
  - V2I
    - Vision_Data_Preprocessing_31.py
    - Position_Data_Preprocessing_31.py
    - 3D_Point_Cloud_Data_Preprocessing_31.py
    - S31-GPS-Google_Map_Plotting.ipynb
    - Proposed-For_Results_Normalized_Power_Calculation.ipynb
    - S31-Fusion_GPS-Visual-LiDAR_mmWave-64-Beams.ipynb
    - csv...
  - V2V
    - Vision_Data_Preprocessing_36.py
    - Position_Data_Preprocessing_36.py
    - 3D_Point_Cloud_Data_Preprocessing_36.py
    - S36-GPS-Google_Map_Plotting.ipynb
    - Proposed-For_Results_Normalized_Power_Calculation.ipynb
    - S36-Fusion_GPS-Visual-LiDAR_mmWave-64-Beams.ipynb
    - csv...

可以从该目录得出几个结论，V2I与V2V所使用的代码基本上是一样的。上述我们已经提到该任务使用场景包括：

- **V2I**（车到基站）：Scenario 31, 32, 33, 34
- **V2V**（车到车）：Scenario 36, 37, 38, 39

作者只开源了场景31与场景36的代码，猜测其他场景使用相同的代码，所以稍后需要进行略微修改。

**下面开始对上述几个文件的功能进行阐述（以V2I为例）：**

1. Vision_Data_Preprocessing_31.py

   用于生成该文件的功能是对场景31中所提供的图像数据进行划分，生成适合于后续机器学习任务的数据集。首先读取原始数据集（CSV 文件），提取图像路径、功率数据文件路径、原始波束索引和样本索引。对每个功率数据文件进行下采样（每隔一个点采样一次），并重新计算最佳波束索引。生成新的图像路径，并将相关信息（索引、图像路径、原始最佳波束、原始功率数据）保存到新的 CSV 文件中。随机划分数据集为训练集、验证集和测试集，并分别保存为不同的 CSV 文件，便于后续模型训练和评估。

2. Position_Data_Preprocessing_31.py

   与Vision脚本功能一样，额外读取每个样本的位置信息（经纬度），并对所有样本的经纬度分别做归一化处理（将其缩放到0~1区间）。

3. 3D_Point_Cloud_Data_Preprocessing_31.py

   与上述脚本主要功能一致。

4. S31-GPS-Google_Map_Plotting.ipynb

   将GPS定位数据（经纬度）可视化地绘制在Google地图上。方便直观展示车辆或设备的运动路径、采样点分布等空间信息。

5. Proposed-For_Results_Normalized_Power_Calculation.ipynb

   对通信系统实验或仿真得到的功率数据进行归一化处理，并计算相关性能指标，用于结果分析和对比。

   - 读取原始功率数据（如不同波束、不同位置下的接收功率）
   - 对功率数据进行归一化（如最大最小归一化、相对最大值归一化等）
   - 计算归一化后的性能指标（如平均归一化功率、归一化增益、波束选择准确率等）
   - 可视化归一化结果，便于对比不同算法或方案的性能

6. S31-Fusion_GPS-Visual-LiDAR_mmWave-64-Beams.ipynb

   融合GPS（位置信息）、视觉（图像）、激光雷达（LiDAR）等多模态数据，结合毫米波（mmWave）64波束通信数据，进行波束选择。

   - 读取和预处理GPS、图像、激光雷达、毫米波功率等多源数据
   - 构建多模态特征输入
   - 训练和评估基于多模态输入的波束选择或信道预测模型
   - 对比不同模态或融合方式下的性能
   - 可视化实验结果