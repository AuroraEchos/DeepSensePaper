#### 主要贡献：

- 首次在真实 V2I 场景中验证视觉辅助波束跟踪的可行性，利用 RGB 图像信息实现低开销、高精度的波束预测。
- 提出端到端计算机视觉辅助波束跟踪模型，结合目标检测（YOLOv4）与时序建模（GRU）进行 beam sequence 预测。
- 在 DeepSense 6G 场景 8 上实测验证，模型在 top-5 精度下达到 **98.95%**，并实现超过 **99%** 的 beam training 开销节省。

#### 采用方法：

- 系统模型：
  - 静态基站（BS）装备 16 阵列天线 + RGB 相机；
  - 用户设备（UE）为 60GHz 发射端，使用 64 波束码本进行波束扫描。
- 数据处理流程：
  - 利用 YOLOv4 提取每帧图像中的车辆 bounding box 特征，作为动态环境表示；
  - 构造时序输入：以 8 帧历史图像作为输入序列，映射为 64 维嵌入向量。
- 模型结构：
  - 编码器：YOLOv4 + FC 映射为时间步嵌入；
  - GRU 模块学习时序特征，解码器输出未来 5 步波束方向预测；
  - 每一步均通过 softmax 分类器输出 top-1 或 top-5 波束索引。

#### 实验结果：

- Top-1 波束预测准确率：**64.47%**
- Top-5 波束预测准确率：**98.95%**
- Top-1 归一化接收功率：**97.66%**
- 预测延迟仅 **1.45ms**，可部署于实际系统中。
- 与 baseline（使用过去 beam 序列）比较，开销下降超过 **99%**。

#### 局限性：

- 仅评估单一场景（Scenario 8）与单用户设置，泛化能力尚待验证；
- 仅采用 RGB 图像，缺乏多模态信息融合（如 LiDAR 或信道特征）；
- YOLOv4 对不同光照和天气条件下的鲁棒性未深入讨论。